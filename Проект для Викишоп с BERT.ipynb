{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19154c75",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Библиотеки\" data-toc-modified-id=\"Библиотеки-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Библиотеки</a></span></li><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Вывод</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Вывод\" data-toc-modified-id=\"Вывод-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Вывод</a></span></li></ul></li><li><span><a href=\"#Финальное-тестирование\" data-toc-modified-id=\"Финальное-тестирование-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Финальное тестирование</a></span><ul class=\"toc-item\"><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Выводы</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10020172",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7142f6d6",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Необходимо обучить модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок. Значение метрики качества *F1* должно быть не меньше 0.75. \n",
    "\n",
    "Постройте модель со значением \n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa045c",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd73420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\igors\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# стандартные библиотеки\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# сторонние библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import nltk\n",
    "\n",
    "# модули\n",
    "from tqdm import notebook\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_val_predict,\n",
    "    train_test_split\n",
    ")\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# настройки\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# константы\n",
    "RANDOM_SATATE = 0\n",
    "\n",
    "PATH_YANDEX = 'datasets/toxic_comments.csv'    \n",
    "PATH = 'C:/Users/igors/Downloads/toxic_comments.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea9038",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5df005",
   "metadata": {},
   "source": [
    "Загрузим данные и осмотрим первые строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5bbc77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.exists(PATH):\n",
    "    data = pd.read_csv(PATH, index_col=[0])\n",
    "    display(data.head(5))\n",
    "elif os.path.exists(PATH_YANDEX):\n",
    "    data = pd.read_csv(PATH_YANDEX, index_col=[0])\n",
    "    display(data.head(5))\n",
    "else:\n",
    "    print('Ошибка в считывании данных') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb42729",
   "metadata": {},
   "source": [
    "Проверим данные на нализичие пропусков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74aeb649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f173c",
   "metadata": {},
   "source": [
    "Пропусков нет."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15bcf1f",
   "metadata": {},
   "source": [
    "Т.к. будем работать с BERT, уменьшим выборку до 1000 объектов, сохранив баланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c69cc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     143106\n",
       "toxic    143106\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['toxic'] == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551a1ebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     16186\n",
       "toxic    16186\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['toxic'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7d212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = data[data['toxic'] == 0].sample(round(143/(143+16)*1000), random_state=RANDOM_SATATE).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8694c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = data[data['toxic'] == 1].sample(round(16/(143+16)*1000), random_state=RANDOM_SATATE).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f890fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_sample = shuffle(pd.concat([zeros, ones]), random_state=RANDOM_SATATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325f08f",
   "metadata": {},
   "source": [
    "Задействуем модель toxic-bert для токенизации и создания эмбеддингов, затем разобьём данные на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac1ee7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at unitary/toxic-bert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
    "model = transformers.BertModel.from_pretrained(\"unitary/toxic-bert\")\n",
    "\n",
    "tokenized = stratified_sample['text'].apply(\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True))\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4ae4443",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b423075f232b4cf4a1282e3277fa1d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'features' (ndarray)\n",
      "Stored 'target' (Series)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "\n",
    "features = np.concatenate(embeddings)\n",
    "target = stratified_sample[:features.shape[0]]['toxic']\n",
    "\n",
    "%store features\n",
    "%store target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd11a4a4",
   "metadata": {},
   "source": [
    "Сохраняем значения features и target на случай нестабильной работы ядра."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4307716",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4b4ce6",
   "metadata": {},
   "source": [
    "В ходе потоготовки\n",
    "- из данных была взята выборка из 1000 объектов с сохранением пропорций классов;\n",
    "- тексты были токенизированы, после чего были подготовлены эмбеддинги с помощью модели toxic-bert;\n",
    "- были сформированы признаки для дальнейшей классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c307044",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf412c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r features\n",
    "%store -r target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94348ac7",
   "metadata": {},
   "source": [
    "Разобьём признаки и целевые признаки на тренировочную и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e999af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.25, random_state=RANDOM_SATATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa477fe",
   "metadata": {},
   "source": [
    "Для решения задачи будем использовать модели LinerRegression, DecisionTreeClassifier, KNeighborsClassifier и RandomForestClassifier.\n",
    "Подберём оптимальные параметры одновременно с оптимальным порогом классификации для каждой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f36dd197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a5bd9110aa433cbbeb7617bfe9d163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c864306dbe9a4eb1a1cf48d81be08543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'class_weight': None,\n",
       " 'penalty': 'l2',\n",
       " 'C': 0.2,\n",
       " 'threshold': 0.7000000000000001,\n",
       " 'F1': 0.951048951048951}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_params = {'class_weight': None, 'penalty': None, 'C': None, 'threshold': None, 'F1': None}\n",
    "best_score = 0\n",
    "\n",
    "for class_weight in [None, 'balanced']:    \n",
    "    for penalty in notebook.tqdm(['l1', 'l2', 'elasticnet', None]):\n",
    "        for C in np.arange(0, 1.1, 0.2):\n",
    "            for threshold in np.arange(0.1, 1.05, 0.1):\n",
    "                try:\n",
    "                    model = LogisticRegression(C=C, penalty=penalty, random_state=RANDOM_SATATE, \n",
    "                                               n_jobs=-1, class_weight=class_weight)\n",
    "                    predict_proba = cross_val_predict(model, train_features, train_target, method='predict_proba')\n",
    "                    predict = [1 if x[1] >= threshold else 0 for x in predict_proba]\n",
    "                    score = f1_score(predict, train_target)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params['class_weight'] = class_weight\n",
    "                        best_params['penalty'] = penalty\n",
    "                        best_params['C'] = C\n",
    "                        best_params['threshold'] = threshold\n",
    "                        best_params['F1'] =  best_score                           \n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "display(best_params)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ad4f6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04fda199c2349ff82b19a58ddbac3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c123c0384414b098dd23d71a816d41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'max_depth': 2,\n",
       " 'threshold': 0.1,\n",
       " 'F1': 0.9395973154362416}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_params_DT = {'class_weight': None, 'max_depth': None, 'threshold': None, 'F1': None}\n",
    "best_score = 0\n",
    "for class_weight in [None, 'balanced']:\n",
    "    for max_depth in notebook.tqdm(range(1, 11)):\n",
    "        for threshold in np.arange(0.1, 1.05, 0.1):\n",
    "            model = DecisionTreeClassifier(max_depth=max_depth, random_state=RANDOM_SATATE, class_weight=class_weight)\n",
    "            predict_proba = cross_val_predict(model, train_features, train_target, method='predict_proba')\n",
    "            predict = [1 if x[1] >= threshold else 0 for x in predict_proba]\n",
    "            score = f1_score(predict, train_target)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params_DT['class_weight'] = class_weight\n",
    "                best_params_DT['max_depth'] = max_depth\n",
    "                best_params_DT['threshold'] = threshold\n",
    "                best_params_DT['F1'] = best_score                           \n",
    "\n",
    "display(best_params_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a62afdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabc6c255b2b4ebab627cca9bd0396ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 3, 'threshold': 0.4, 'F1': 0.9583333333333333}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_params_KN = {'n_neighbors': None, 'threshold': None, 'F1': None}\n",
    "best_score = 0\n",
    "for n_neighbors in notebook.tqdm(range(1, 11)):\n",
    "    for threshold in np.arange(0.1, 1.05, 0.1):\n",
    "        model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        predict_proba = cross_val_predict(model, train_features, train_target, method='predict_proba')\n",
    "        predict = [1 if x[1] >= threshold else 0 for x in predict_proba]\n",
    "        score = f1_score(predict, train_target)\n",
    "        if score > best_score:\n",
    "            best_score = score \n",
    "            best_params_KN['n_neighbors'] = n_neighbors\n",
    "            best_params_KN['threshold'] = threshold\n",
    "            best_params_KN['F1'] = best_score\n",
    "            \n",
    "display(best_params_KN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "478cf62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3634fd850dc44339a4b5c30b35327919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b46cd5ab2941c6b3186ea68454722f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'n_estimators': 10,\n",
       " 'max_depth': 2,\n",
       " 'threshold': 0.6,\n",
       " 'F1': 0.9589041095890412}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_params_RF = {'class_weight': None, 'n_estimators': None, 'max_depth': None, 'threshold': None, 'F1': None}\n",
    "best_score = 0\n",
    "for class_weight in [None, 'balanced']:    \n",
    "    for n_estimators in notebook.tqdm(range(10, 210, 50)):\n",
    "        for max_depth in range(1, 11):\n",
    "            for threshold in np.arange(0.1, 1.05, 0.1):\n",
    "                model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=RANDOM_SATATE, class_weight=class_weight)\n",
    "                predict_proba = cross_val_predict(model, train_features, train_target, method='predict_proba')\n",
    "                predict = [1 if x[1] >= threshold else 0 for x in predict_proba]\n",
    "                score = f1_score(predict, train_target)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params_RF['class_weight'] = class_weight\n",
    "                    best_params_RF['n_estimators'] = n_estimators\n",
    "                    best_params_RF['max_depth'] = max_depth\n",
    "                    best_params_RF['threshold'] = threshold\n",
    "                    best_params_RF['F1'] = best_score                           \n",
    "\n",
    "display(best_params_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc479f",
   "metadata": {},
   "source": [
    "Все модели показали отличные результаты, тем не менее самое высокое значение F1-меры у RandomForestClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c74a3",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504c5b0",
   "metadata": {},
   "source": [
    "В ходе обучения \n",
    "- были подобраны оптимальные гиперпараметры и оптимальные пороги классификации для каждой модели\n",
    "- было принято решение использовать в дальнейшем модель **RandomForestClassifier с гиперпараметрами `n_estimators` = 10, `max_depth` = 2, `class_weight` = 'balanced' и порогом классификации 0.6**, т.к. она показывает самые высокие значения F1-меры среди всех рассмотренных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e492646",
   "metadata": {},
   "source": [
    "## Финальное тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7aa901",
   "metadata": {},
   "source": [
    "Напишем класс для предсказаний со сдвигом порога классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1654a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedThresholdModel():\n",
    "    \n",
    "    \n",
    "    def __init__(self, model, threshold):\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predict_proba = pd.Series([x[0] for x in self.model.predict_proba(X)])\n",
    "        return predict_proba.apply(lambda x: 0 if x >= self.threshold else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced05db8",
   "metadata": {},
   "source": [
    "Рассчитаем значение F1-меры на тестовой выбоорке для рекомендуемой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1519d635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526316"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_model = RandomForestClassifier(n_estimators=10, max_depth=2, class_weight='balanced', \n",
    "                               random_state=RANDOM_SATATE, n_jobs=-1\n",
    "                               ).fit(train_features, train_target)\n",
    "threshold = 0.6\n",
    "final_model = ShiftedThresholdModel(pre_model, threshold)\n",
    "f1_score(test_target, final_model.predict(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599fc8d",
   "metadata": {},
   "source": [
    "### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f703919",
   "metadata": {},
   "source": [
    "В результате финального тестирования значение F1-метрики рекомендуемой модели равно 0.95, что удовлетворяет условиям поставленной задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e657ce6",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a70d2",
   "metadata": {},
   "source": [
    "В ходе потоготовки\n",
    "- из данных была взята выборка из 1000 объектов с сохранением пропорций классов;\n",
    "- тексты были токенизированы, после чего были подготовлены эмбеддинги с помощью модели DistilBert;\n",
    "- были сформированы признаки для дальнейшей классификации.\n",
    "\n",
    "В ходе обучения \n",
    "- были подобраны оптимальные гиперпараметры и оптимальные пороги классификации для каждой модели\n",
    "- было принято решение использовать в дальнейшем модель **RandomForestClassifier с гиперпараметрами `n_estimators` = 10, `max_depth` = 2, `class_weight` = 'balanced' и порогом классификации 0.6**, т.к. она показывает самые высокие значения F1-меры среди всех рассмотренных моделей.\n",
    "\n",
    "В результате финального тестирования значение F1-меры рекомендуемой модели равно 0.95, что удовлетворяет условиям поставленной задачи."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Содержание",
   "toc_cell": true,
   "toc_position": {
    "height": "541.6px",
    "left": "29px",
    "top": "110.525px",
    "width": "257.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
